{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This ResNet code from https://github.com/megvii-research/mdistiller/blob/master/mdistiller/models/cifar/resnet.py\n",
    "'''\n",
    "\n",
    "__all__ = [\"resnet\"]\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    '''\n",
    "    3 x 3 convolution with padding\n",
    "    '''\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    BasicBlock : Conv층 2개로 이루어지며, 잔차가 포함된 block\n",
    "    '''\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True) #inplace=True로 하면 들어가는 인수 값이 output과 동일하게 변동, 메모리 절약 효과\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Downsample이 필요하다면?\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = F.relu(out) #init에 쓰는 nn.ReLU와 다르게 forward에서는 F.relu 쓰인다고함\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_filters, block_name=\"BasicBlock\", num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "    \n",
    "        if block_name.lower() == \"basicblock\":\n",
    "            assert(\n",
    "                depth - 2\n",
    "            ) % 6 == 0, \"Basic block depth should be 6n+2, 20, 32, 44, 56, 110 등\"\n",
    "            n = (depth - 2) // 6\n",
    "            block = BasicBlock\n",
    "        else:\n",
    "            raise ValueError(\"block_name should be Basicblock\")\n",
    "        \n",
    "        self.inplanes = num_filters[0]\n",
    "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, num_filters[1], n)\n",
    "        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n",
    "        self.stage_channels = num_filters\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = list([])\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, is_last=(blocks == 1))\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, is_last=(i == blocks - 1)))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def get_feat_modules(self):\n",
    "        feat_m = nn.ModuleList([])\n",
    "        feat_m.append(self.conv1)\n",
    "        feat_m.append(self.bn1)\n",
    "        feat_m.append(self.relu)\n",
    "        feat_m.append(self.layer1)\n",
    "        feat_m.append(self.layer2)\n",
    "        feat_m.append(self.layer3)\n",
    "        return feat_m\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        f0 = x\n",
    "\n",
    "        x, f1_pre = self.layer1(x)\n",
    "        f1 = x\n",
    "        x, f2_pre = self.layer2(x)\n",
    "        f2 = x\n",
    "        x, f3_pre = self.layer3(x)\n",
    "        f3 = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        avg = x.reshape(x.size(0), -1)\n",
    "        out = self.fc(avg)\n",
    "\n",
    "        feats = {}\n",
    "        feats[\"feats\"] = [f0, f1, f2, f3]\n",
    "        feats[\"preact_feats\"] = [f0, f1_pre, f2_pre, f3_pre]\n",
    "        feats[\"pooled_feat\"] = avg\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet8(**kwargs):\n",
    "    return ResNet(8, [16, 16, 32, 64], \"basicblock\", **kwargs), \"resnet8\"\n",
    "\n",
    "def resnet20(**kwargs):\n",
    "    return ResNet(20, [16, 16, 32, 64], \"basicblock\", **kwargs), \"resnet20\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(bz=64):\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "        ]\n",
    "    )\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "        ]\n",
    "    )\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./../../data', train=True, download=True, transform=train_transform)\n",
    "    testset = torchvision.datasets.CIFAR100(root='./../../data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=bz, shuffle=True, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=bz, shuffle=True, num_workers=0)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "seed = 2021\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device : {device}\")\n",
    "train_loader, test_loader = load_dataset()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "student, student_name = resnet8(num_classes=100)\n",
    "teacher, teacher_name = resnet20(num_classes=100)\n",
    "\n",
    "student.to(device)\n",
    "teacher.to(device)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=0.05, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.train()\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    batch_size = inputs.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_stu = student(inputs)\n",
    "    output_tea = teacher(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance-wise Distillation loss\n",
    "\n",
    "def instance_distill_loss(student_logits, teacher_logits):\n",
    "    # L2 norm이라 p=2라고 설정, L1 norm이면 p=1 \n",
    "    student_norm = F.normalize(student_logits, p=2, dim=1)\n",
    "    teacher_norm = F.normalize(teacher_logits, p=2, dim=1)\n",
    "    \n",
    "    instance_loss = F.mse_loss(student_norm, teacher_norm)\n",
    "    \n",
    "    return instance_loss\n",
    "    \n",
    "# class-wise distillation loss\n",
    "def class_distill_loss(t_student_logits, t_teacher_logits):\n",
    "    t_student_norm = F.normalize(t_student_logits, p=2, dim=1)\n",
    "    t_teacher_norm = F.normalize(t_teacher_logits, p=2, dim=1)\n",
    "    \n",
    "    class_loss = F.mse_loss(t_student_norm, t_teacher_norm)\n",
    "    \n",
    "    return class_loss\n",
    "\n",
    "# class correlation loss \n",
    "def class_correlation_loss(student_logits, teacher_logits):\n",
    "    '''\n",
    "    1. Class Correlation Matrix\n",
    "    2. Frobenius Norm(L2 norm)\n",
    "    '''\n",
    "    # 1. Class Correlation Matrix\n",
    "    N, C = student_logits.shape\n",
    "    \n",
    "    student_mean = torch.mean(student_logits, dim=0)\n",
    "    teacher_mean = torch.mean(teacher_logits, dim=0)\n",
    "    \n",
    "    B_s, B_t = torch.zeros((N, N)).to(device), torch.zeros((N, N)).to(device)\n",
    "    for j in range(C):\n",
    "        student_j = student_logits[:, j]\n",
    "        diff_s = student_j - student_mean[j]\n",
    "        B_s += torch.outer(torch.t(diff_s), diff_s)\n",
    "        \n",
    "        teacher_j = teacher_logits[:, j]\n",
    "        diff_t = teacher_j - teacher_mean[j]\n",
    "        B_t += torch.outer(torch.t(diff_t), diff_t)\n",
    "    \n",
    "    B_s /= (C-1)\n",
    "    B_t /= (C-1)\n",
    "    \n",
    "    # 2. Frobenius Norm(L2 norm)\n",
    "    diff = B_s - B_t\n",
    "    diff_norm = torch.norm(diff, 'fro') # Frobenius Norm\n",
    "    class_corr_loss = (1 / (C**2)) * diff_norm ** 2\n",
    "    \n",
    "    return class_corr_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb, mu, nu = 0.1, 0.5, 0.4\n",
    "t_output_stu = torch.t(output_stu)\n",
    "t_output_tea = torch.t(output_tea)\n",
    "ins = instance_distill_loss(output_stu, output_tea)\n",
    "cla = class_distill_loss(t_output_stu, t_output_tea)\n",
    "clcor = class_correlation_loss(output_stu, output_tea)\n",
    "total_loss = ins * lamb + cla * mu + clcor * nu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
